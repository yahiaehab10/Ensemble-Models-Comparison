{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GlassTypePrediction\n",
    "Responsibility : Mariam Amr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For implementing ensemble models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "\n",
    "# For hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./AMLAss1Datasets/glasstypePrediction.csv\"\n",
    ")  # Suitable for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* Dataset has no nulls\n",
    "* All columns will be used in our project\n",
    "* Scaling the numerical features \n",
    "    ```python\n",
    "    StandardScaler() #Scaling data to fit a standard normal distribution\n",
    "    ```\n",
    "* Removing outliers using `IQR` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "df.columns = [\n",
    "    \"ri\", \"na\", \"mg\", \"al\", \"si\", \"k\", \"ca\", \"ba\", \"fe\", \"type\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numerical features\n",
    "scaler = StandardScaler()\n",
    "df[[\"ri\", \"na\", \"mg\", \"al\", \"si\", \"k\", \"ca\", \"ba\", \"fe\"]] = scaler.fit_transform(\n",
    "    df[[\"ri\", \"na\", \"mg\", \"al\", \"si\", \"k\", \"ca\", \"ba\", \"fe\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ri</th>\n",
       "      <th>na</th>\n",
       "      <th>mg</th>\n",
       "      <th>al</th>\n",
       "      <th>si</th>\n",
       "      <th>k</th>\n",
       "      <th>ca</th>\n",
       "      <th>ba</th>\n",
       "      <th>fe</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.872868</td>\n",
       "      <td>0.284953</td>\n",
       "      <td>1.254639</td>\n",
       "      <td>-0.692442</td>\n",
       "      <td>-1.127082</td>\n",
       "      <td>-0.671705</td>\n",
       "      <td>-0.145766</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.249333</td>\n",
       "      <td>0.591817</td>\n",
       "      <td>0.636168</td>\n",
       "      <td>-0.170460</td>\n",
       "      <td>0.102319</td>\n",
       "      <td>-0.026213</td>\n",
       "      <td>-0.793734</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.721318</td>\n",
       "      <td>0.149933</td>\n",
       "      <td>0.601422</td>\n",
       "      <td>0.190912</td>\n",
       "      <td>0.438787</td>\n",
       "      <td>-0.164533</td>\n",
       "      <td>-0.828949</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.232831</td>\n",
       "      <td>-0.242853</td>\n",
       "      <td>0.698710</td>\n",
       "      <td>-0.310994</td>\n",
       "      <td>-0.052974</td>\n",
       "      <td>0.112107</td>\n",
       "      <td>-0.519052</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.312045</td>\n",
       "      <td>-0.169205</td>\n",
       "      <td>0.650066</td>\n",
       "      <td>-0.411375</td>\n",
       "      <td>0.555256</td>\n",
       "      <td>0.081369</td>\n",
       "      <td>-0.624699</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ri        na        mg        al        si         k        ca  \\\n",
       "0  0.872868  0.284953  1.254639 -0.692442 -1.127082 -0.671705 -0.145766   \n",
       "1 -0.249333  0.591817  0.636168 -0.170460  0.102319 -0.026213 -0.793734   \n",
       "2 -0.721318  0.149933  0.601422  0.190912  0.438787 -0.164533 -0.828949   \n",
       "3 -0.232831 -0.242853  0.698710 -0.310994 -0.052974  0.112107 -0.519052   \n",
       "4 -0.312045 -0.169205  0.650066 -0.411375  0.555256  0.081369 -0.624699   \n",
       "\n",
       "         ba        fe  type  \n",
       "0 -0.352877 -0.586451     1  \n",
       "1 -0.352877 -0.586451     1  \n",
       "2 -0.352877 -0.586451     1  \n",
       "3 -0.352877 -0.586451     1  \n",
       "4 -0.352877 -0.586451     1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation & Hyperparameter Turning\n",
    "\n",
    "References :\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "y = df[\"type\"]\n",
    "X = df.drop(\"type\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#random_state here is used to randomly split the data into train and test sets (randomize data before split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier default parameters:\n",
      "Accuracy: 0.8372093023255814\n",
      "Precision: 0.9127314814814816\n",
      "Recall: 0.8432539682539683\n",
      "F1 Score: 0.8605223570909845\n"
     ]
    }
   ],
   "source": [
    "#random_state here and in the other models introduces randomness when splitting and selecting the feature subsets but also when comparing models, setting random_state to the same value will make the comparison fair (instead of using internal randomness)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "#Performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred)\n",
    "precision_rf = precision_score(y_test, y_pred, average=\"macro\",zero_division=1)\n",
    "recall_rf = recall_score(y_test, y_pred, average=\"macro\",zero_division=1)\n",
    "f1_rf = f1_score(y_test, y_pred, average=\"macro\",zero_division=1)\n",
    "\n",
    "print(\"Random Forest Classifier default parameters:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Precision:\", precision_rf)\n",
    "print(\"Recall:\", recall_rf)\n",
    "print(\"F1 Score:\", f1_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest Classifier:\n",
      "{'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning with RandomSearchCV\n",
    "params={\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5,10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_random = RandomizedSearchCV(rf_model, param_distributions=params, n_iter=100, cv=5)\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Random Forest Classifier:\")\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest Classifier from Grid Search:\n",
      "{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning with GridSearchCV\n",
    "rf_grid = GridSearchCV(rf_model, param_grid=params, cv=5, n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Random Forest Classifier from Grid Search:\")\n",
    "print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier tuned parameters:\n",
      "Accuracy: 0.8604651162790697\n",
      "Precision: 0.9299145299145298\n",
      "Recall: 0.8253968253968255\n",
      "F1 Score: 0.8546058879392211\n",
      "\n",
      "\n",
      "Random Forest Classifier default parameters:\n",
      "Accuracy: 0.8372093023255814\n",
      "Precision: 0.9127314814814816\n",
      "Recall: 0.8432539682539683\n",
      "F1 Score: 0.8605223570909845\n"
     ]
    }
   ],
   "source": [
    "#Final Comparison between initial and tuned models\n",
    "\n",
    "#Tuned model\n",
    "rf_model_tuned = RandomForestClassifier(\n",
    "    n_estimators=150,\n",
    "    criterion='gini',\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    ")\n",
    "rf_model_tuned.fit(X_train, y_train)\n",
    "y_pred_tuned = rf_model_tuned.predict(X_test)\n",
    "\n",
    "accuracy_rf_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_rf_tuned = precision_score(y_test, y_pred_tuned, average=\"macro\",zero_division=1)\n",
    "recall_rf_tuned = recall_score(y_test, y_pred_tuned, average=\"macro\",zero_division=1)\n",
    "f1_rf_tuned = f1_score(y_test, y_pred_tuned, average=\"macro\",zero_division=1)\n",
    "\n",
    "print(\"Random Forest Classifier tuned parameters:\")\n",
    "print(\"Accuracy:\", accuracy_rf_tuned)\n",
    "print(\"Precision:\", precision_rf_tuned)\n",
    "print(\"Recall:\", recall_rf_tuned)\n",
    "print(\"F1 Score:\", f1_rf_tuned)\n",
    "\n",
    "#Initial model\n",
    "print(\"\\n\")\n",
    "print(\"Random Forest Classifier default parameters:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Precision:\", precision_rf)\n",
    "print(\"Recall:\", recall_rf)\n",
    "print(\"F1 Score:\", f1_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier default parameters performance:\n",
      "Accuracy: 0.4883720930232558\n",
      "Precision: 0.7671568627450981\n",
      "Recall: 0.43127705627705626\n",
      "F1 Score: 0.3815668202764977\n"
     ]
    }
   ],
   "source": [
    "ab_model = AdaBoostClassifier(random_state=42)\n",
    "ab_model.fit(X_train, y_train)\n",
    "y_pred_ab = ab_model.predict(X_test)\n",
    "\n",
    "#Performance\n",
    "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
    "precision_ab = precision_score(y_test, y_pred_ab, average=\"macro\",zero_division=1)\n",
    "recall_ab = recall_score(y_test, y_pred_ab, average=\"macro\",zero_division=1)\n",
    "f1_ab = f1_score(y_test, y_pred_ab, average=\"macro\",zero_division=1)\n",
    "\n",
    "print(\"AdaBoost Classifier default parameters performance:\")\n",
    "print(\"Accuracy:\", accuracy_ab)\n",
    "print(\"Precision:\", precision_ab)\n",
    "print(\"Recall:\", recall_ab)\n",
    "print(\"F1 Score:\", f1_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for AdaBoost Classifier with Random Search:\n",
      "{'n_estimators': 150, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning with RandomSearchCV\n",
    "params={\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ab_random = RandomizedSearchCV(ab_model, param_distributions=params, n_iter=100, cv=5)\n",
    "ab_random.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for AdaBoost Classifier with Random Search:\")\n",
    "print(ab_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for AdaBoost Classifier from Grid Search:\n",
      "{'learning_rate': 0.1, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning with GridSearchCV\n",
    "ab_grid = GridSearchCV(ab_model, param_grid=params, cv=5)\n",
    "ab_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for AdaBoost Classifier from Grid Search:\")\n",
    "print(ab_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier tuned parameters:\n",
      "Accuracy: 0.6976744186046512\n",
      "Precision: 0.7705415499533146\n",
      "Recall: 0.6006493506493507\n",
      "F1 Score: 0.5954224270353302\n",
      "\n",
      "\n",
      "AdaBoost Classifier default parameters performance:\n",
      "Accuracy: 0.4883720930232558\n",
      "Precision: 0.7671568627450981\n",
      "Recall: 0.43127705627705626\n",
      "F1 Score: 0.3815668202764977\n"
     ]
    }
   ],
   "source": [
    "#Final Comparison between initial and tuned models\n",
    "\n",
    "#Tuned model\n",
    "ab_model_tuned = AdaBoostClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "ab_model_tuned.fit(X_train, y_train)\n",
    "ab_y_pred_tuned = ab_model_tuned.predict(X_test)\n",
    "\n",
    "accuracy_ab_tuned = accuracy_score(y_test, ab_y_pred_tuned)\n",
    "precision_ab_tuned = precision_score(y_test, ab_y_pred_tuned, average=\"macro\",zero_division=1)\n",
    "recall_ab_tuned = recall_score(y_test, ab_y_pred_tuned, average=\"macro\",zero_division=1)\n",
    "f1_ab_tuned = f1_score(y_test, ab_y_pred_tuned, average=\"macro\",zero_division=1)\n",
    "\n",
    "print(\"AdaBoost Classifier tuned parameters:\")\n",
    "print(\"Accuracy:\", accuracy_ab_tuned)\n",
    "print(\"Precision:\", precision_ab_tuned)\n",
    "print(\"Recall:\", recall_ab_tuned)\n",
    "print(\"F1 Score:\", f1_ab_tuned)\n",
    "\n",
    "#Initial model\n",
    "print(\"\\n\")\n",
    "print(\"AdaBoost Classifier default parameters performance:\")\n",
    "print(\"Accuracy:\", accuracy_ab)\n",
    "print(\"Precision:\", precision_ab)\n",
    "print(\"Recall:\", recall_ab)\n",
    "print(\"F1 Score:\", f1_ab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier default parameters performance:\n",
      "Accuracy: 0.8604651162790697\n",
      "Precision: 0.8902116402116403\n",
      "Recall: 0.8115079365079364\n",
      "F1 Score: 0.813602671837966\n"
     ]
    }
   ],
   "source": [
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_y_pred = gb_model.predict(X_test)\n",
    "\n",
    "#Performance\n",
    "accuracy_gb = accuracy_score(y_test, gb_y_pred)\n",
    "precision_gb = precision_score(y_test, gb_y_pred, average=\"macro\",zero_division=1)\n",
    "recall_gb = recall_score(y_test, gb_y_pred, average=\"macro\",zero_division=1)\n",
    "f1_gb = f1_score(y_test, gb_y_pred, average=\"macro\",zero_division=1)\n",
    "\n",
    "print(\"Gradient Boosting Classifier default parameters performance:\")\n",
    "print(\"Accuracy:\", accuracy_gb)\n",
    "print(\"Precision:\", precision_gb)\n",
    "print(\"Recall:\", recall_gb)\n",
    "print(\"F1 Score:\", f1_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting Classifier with Random Search:\n",
      "{'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': 7, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning with RandomSearchCV\n",
    "params={\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1,1.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "gb_random = RandomizedSearchCV(gb_model, param_distributions=params, n_iter=100, cv=5)\n",
    "gb_random.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Gradient Boosting Classifier with Random Search:\")\n",
    "print(gb_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Hyperparameter Tuning with GridSearchCV\u001b[39;00m\n\u001b[0;32m      2\u001b[0m gb_grid \u001b[38;5;241m=\u001b[39m GridSearchCV(gb_model, param_grid\u001b[38;5;241m=\u001b[39mparams, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mgb_grid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for Gradient Boosting Classifier from Grid Search:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(gb_grid\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    607\u001b[0m         )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:221\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mis_multi_class:\n\u001b[0;32m    219\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(original_y \u001b[38;5;241m==\u001b[39m k, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m--> 221\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_predictions_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# induce regression tree on residuals\u001b[39;00m\n\u001b[0;32m    226\u001b[0m tree \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(\n\u001b[0;32m    227\u001b[0m     criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion,\n\u001b[0;32m    228\u001b[0m     splitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m     ccp_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mccp_alpha,\n\u001b[0;32m    238\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py:822\u001b[0m, in \u001b[0;36mMultinomialDeviance.negative_gradient\u001b[1;34m(self, y, raw_predictions, k, **kwargs)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnegative_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, raw_predictions, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    807\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute negative gradient for the ``k``-th class.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;124;03m        The index of the class.\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(\n\u001b[1;32m--> 822\u001b[0m         np\u001b[38;5;241m.\u001b[39mexp(raw_predictions[:, k] \u001b[38;5;241m-\u001b[39m \u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    823\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\special\\_logsumexp.py:100\u001b[0m, in \u001b[0;36mlogsumexp\u001b[1;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[0;32m     97\u001b[0m         a \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.\u001b[39m  \u001b[38;5;66;03m# promote to at least float\u001b[39;00m\n\u001b[0;32m     98\u001b[0m         a[b \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m--> 100\u001b[0m a_max \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a_max\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    103\u001b[0m     a_max[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misfinite(a_max)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2827\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2814\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2816\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2817\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2818\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2825\u001b[0m \u001b[38;5;124;03m    ndarray.max : equivalent method\u001b[39;00m\n\u001b[0;32m   2826\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2828\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\El-Wattaneya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Hyperparameter Tuning with GridSearchCV\n",
    "gb_grid = GridSearchCV(gb_model, param_grid=params, cv=5)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Gradient Boosting Classifier from Grid Search:\")\n",
    "print(gb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Comparison between initial and tuned models\n",
    "\n",
    "#Tuned model\n",
    "gb_model_tuned = GradientBoostingClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    ")\n",
    "gb_model_tuned.fit(X_train, y_train)\n",
    "gb_y_pred_tuned = gb_model_tuned_gs.predict(X_test)\n",
    "\n",
    "accuracy_gb_tuned = accuracy_score(y_test, gb_y_pred_tuned)\n",
    "precision_gb_tuned = precision_score(y_test, gb_y_pred_tuned, average=\"macro\",zero_division=1)\n",
    "recall_gb_tuned = recall_score(y_test, gb_y_pred_tuned, average=\"macro\",zero_division=1)\n",
    "f1_gb_tuned = f1_score(y_test, gb_y_pred_tuned, average=\"macro\",zero_division=1)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Gradient Boosting Classifier tuned parameters:\")\n",
    "print(\"Accuracy:\", accuracy_gb_tuned)\n",
    "print(\"Precision:\", precision_gb_tuned)\n",
    "print(\"Recall:\", recall_gb_tuned)\n",
    "print(\"F1 Score:\", f1_gb_tuned)\n",
    "\n",
    "#Initial model\n",
    "print(\"\\n\")\n",
    "print(\"Gradient Boosting Classifier default parameters performance:\")\n",
    "print(\"Accuracy:\", accuracy_gb)\n",
    "print(\"Precision:\", precision_gb)\n",
    "print(\"Recall:\", recall_gb)\n",
    "print(\"F1 Score:\", f1_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the performance of the models\n",
    "models = [\n",
    "    \"Random Forest Classifier\",\n",
    "    \"Random Forest Classifier with best hyperparameters\",\n",
    "    \"AdaBoost Classifier\",\n",
    "    \"AdaBoost Classifier with best hyperparameters\",\n",
    "    \"Gradient Boosting Classifier\",\n",
    "    \"Gradient Boosting Classifier with best hyperparameters\",\n",
    "]\n",
    "\n",
    "accuracies = [\n",
    "    accuracy_score(y_test, rf_model.predict(X_test)),\n",
    "    accuracy_score(y_test, rf_model_tuned.predict(X_test)),\n",
    "    accuracy_score(y_test, ab_model.predict(X_test)),\n",
    "    accuracy_score(y_test, ab_model_tuned.predict(X_test)),\n",
    "    accuracy_score(y_test, gb_model.predict(X_test)),\n",
    "    accuracy_score(y_test, gb_model_tuned.predict(X_test)),\n",
    "]\n",
    "\n",
    "precisions = [\n",
    "    precision_score(y_test, rf_model.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    precision_score(y_test, rf_model_tuned.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    precision_score(y_test, ab_model.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    precision_score(y_test, ab_model_tuned.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    precision_score(y_test, gb_model.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    precision_score(y_test, gb_model_tuned.predict(X_test), average=\"macro\",zero_division=1),\n",
    "]\n",
    "\n",
    "recalls = [\n",
    "    recall_score(y_test, rf_model.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    recall_score(y_test, rf_model_tuned.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    recall_score(y_test, ab_model.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    recall_score(y_test, ab_model_tuned.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    recall_score(y_test, gb_model.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    recall_score(y_test, gb_model_tuned.predict(X_test), average=\"macro\",zero_division=1),\n",
    "]\n",
    "\n",
    "f1_scores = [\n",
    "    f1_score(y_test, rf_model.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    f1_score(y_test, rf_model_tuned.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    f1_score(y_test, ab_model.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    f1_score(y_test, ab_model_tuned.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    f1_score(y_test, gb_model.predict(X_test), average=\"macro\",zero_division=1),\n",
    "    f1_score(y_test, gb_model_tuned.predict(X_test), average=\"macro\",zero_division=1),\n",
    "]\n",
    "\n",
    "model_comparison = pd.DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"Accuracy\": accuracies,\n",
    "    \"Precision\": precisions,\n",
    "    \"Recall\": recalls,\n",
    "    \"F1 Score\": f1_scores,\n",
    "})\n",
    "\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison.plot(kind=\"bar\", x=\"Model\", figsize=(10, 6))\n",
    "\n",
    "plt.title(\"Glass Type Prediction Model Comparison\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
